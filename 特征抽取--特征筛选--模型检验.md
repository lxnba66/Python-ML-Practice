
# 3.1.1 特征提升

## 特征抽取

### 代码55：DictVectorizer对使用字典存储的数据进行特征抽取与量化


```python
# 定义一组字典列表，用来表示多个数据样本（每个字典代表一个数据样本）
measurements = [{'city': 'Dubai', 'temperature': 33.}, {'city': 'London', 'temperature': 12.}, {'city': 'San Fransisco', 'temperature': 18.}]
# 导入DictVectorizer
from sklearn.feature_extraction import DictVectorizer
# 初始化DictVectorizer特征抽取器
vec = DictVectorizer()
# 输出转化之后的特征矩阵
print(vec.fit_transform(measurements).toarray())
# 输出各维度的特征含义
print(vec.get_feature_names())
```

    [[ 1.  0.  0. 33.]
     [ 0.  1.  0. 12.]
     [ 0.  0.  1. 18.]]
    ['city=Dubai', 'city=London', 'city=San Fransisco', 'temperature']
    

***除了字典格式存储的文本数据以外，大部分的文本数据则表现的更为原始，几乎没有使用特殊的数据结构进行存储，只是一系列字符串。处理这样的数据，比较常用的文本特征表示方法为词袋法（Bags of Words）:不考虑这些词语出现的顺序，只是将训练文本中的每个出现过的词汇单独视作一列特征。我们称这些不重复的词汇集合为词表（Vocabulary），于是每条训练文本都可以在高维度的词表上映射出一个特征向量。而特征数值的常见计算方式有两种，分别是：Count Vectorizer 和 TfidfVectorizer。第一种只考虑出现频率，第二种考虑出现频率和倒数，可以压制常用词汇，提升模型性能。停用词（stop words）***

### 代码56：使用CountVectorizer并且不去掉停用词的条件下，对文本特征进行量化的朴素贝叶斯分类性能测试


```python
from sklearn.datasets import fetch_20newsgroups
#从互联网上即时下载新闻样本，subset = 'all'参数表示下载全部近2万条文本
news = fetch_20newsgroups(subset = 'all')

#分割数据集
from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(news.data, news.target, test_size = 0.25, random_state = 33)

# 导入CountVectorizer
from sklearn.feature_extraction.text import CountVectorizer
# 采用默认的配置对CountVectorizer进行初始化（默认配置不去除英文停用词）
count_vec = CountVectorizer()

# 只使用词频统计的方式将原始训练和测试文本转化为特征向量
X_count_train = count_vec.fit_transform(X_train)
X_count_test = count_vec.transform(X_test)

# 导入朴素贝叶斯分类器，使用默认的配置对分类器进行初始化
from sklearn.naive_bayes import MultinomialNB
mnb_count = MultinomialNB()

#使用朴素贝叶斯分类器对CountVectorizer(不去除停用词)后的训练样本进行参数学习
mnb_count.fit(X_count_train, y_train)

# 输出模型准确性结果
print('The accuracy of classifying 20newsgroups using Navie Bayes (CountVectorizer without filtering stopwords):', mnb_count.score(X_count_test, y_test))

# 存储预测结果
y_count_predict = mnb_count.predict(X_count_test)

# 输出更加详细的其他评价分类性能的指标
from sklearn.metrics import classification_report
print(classification_report(y_test, y_count_predict, target_names = news.target_names))

```

    The accuracy of classifying 20newsgroups using Navie Bayes (CountVectorizer without filtering stopwords): 0.8397707979626485
                              precision    recall  f1-score   support
    
                 alt.atheism       0.86      0.86      0.86       201
               comp.graphics       0.59      0.86      0.70       250
     comp.os.ms-windows.misc       0.89      0.10      0.17       248
    comp.sys.ibm.pc.hardware       0.60      0.88      0.72       240
       comp.sys.mac.hardware       0.93      0.78      0.85       242
              comp.windows.x       0.82      0.84      0.83       263
                misc.forsale       0.91      0.70      0.79       257
                   rec.autos       0.89      0.89      0.89       238
             rec.motorcycles       0.98      0.92      0.95       276
          rec.sport.baseball       0.98      0.91      0.95       251
            rec.sport.hockey       0.93      0.99      0.96       233
                   sci.crypt       0.86      0.98      0.91       238
             sci.electronics       0.85      0.88      0.86       249
                     sci.med       0.92      0.94      0.93       245
                   sci.space       0.89      0.96      0.92       221
      soc.religion.christian       0.78      0.96      0.86       232
          talk.politics.guns       0.88      0.96      0.92       251
       talk.politics.mideast       0.90      0.98      0.94       231
          talk.politics.misc       0.79      0.89      0.84       188
          talk.religion.misc       0.93      0.44      0.60       158
    
                 avg / total       0.86      0.84      0.82      4712
    
    

### 代码57：使用TfidfVectorizer并且不去掉停用词的条件下，对文本特征进行量化的朴素贝叶斯分类性能测试


```python
from sklearn.feature_extraction.text import TfidfVectorizer
# 采用默认配置对TfidfVectorizer进行初始化（默认配置不去除英文停用词）
tfidf_vec = TfidfVectorizer()

#使用tfidf的方式，将原始训练和测试文本转化为特征向量
X_tfidf_train = tfidf_vec.fit_transform(X_train)
X_tfidf_test = tfidf_vec.transform(X_test)

#依然使用默认配置的朴素贝叶斯分类器，在相同的训练和测试数据上，对新的特征量化方式进行性能评估
mnb_tfidf = MultinomialNB()
mnb_tfidf.fit(X_tfidf_train, y_train)
print('The accuracy of classifying 20newsgroups using Navie Bayes (TfidfVectorizer without filtering stopwords):', mnb_tfidf.score(X_tfidf_test, y_test))
y_tfidf_predict = mnb_tfidf.predict(X_tfidf_test)
print(classification_report(y_test, y_tfidf_predict, target_names = news.target_names))
```

    The accuracy of classifying 20newsgroups using Navie Bayes (TfidfVectorizer without filtering stopwords): 0.8463497453310697
                              precision    recall  f1-score   support
    
                 alt.atheism       0.84      0.67      0.75       201
               comp.graphics       0.85      0.74      0.79       250
     comp.os.ms-windows.misc       0.82      0.85      0.83       248
    comp.sys.ibm.pc.hardware       0.76      0.88      0.82       240
       comp.sys.mac.hardware       0.94      0.84      0.89       242
              comp.windows.x       0.96      0.84      0.89       263
                misc.forsale       0.93      0.69      0.79       257
                   rec.autos       0.84      0.92      0.88       238
             rec.motorcycles       0.98      0.92      0.95       276
          rec.sport.baseball       0.96      0.91      0.94       251
            rec.sport.hockey       0.88      0.99      0.93       233
                   sci.crypt       0.73      0.98      0.83       238
             sci.electronics       0.91      0.83      0.87       249
                     sci.med       0.97      0.92      0.95       245
                   sci.space       0.89      0.96      0.93       221
      soc.religion.christian       0.51      0.97      0.67       232
          talk.politics.guns       0.83      0.96      0.89       251
       talk.politics.mideast       0.92      0.97      0.95       231
          talk.politics.misc       0.98      0.62      0.76       188
          talk.religion.misc       0.93      0.16      0.28       158
    
                 avg / total       0.87      0.85      0.84      4712
    
    

### 代码58：分别使用CountVectorizer与TfidfVectorizer，并且使用去除停用词条件下，对文本特征进行量化的朴素贝叶斯分类器性能测试


```python
# 分别使用停用词过滤配置初始化CountVectorizer和TfidfVectorizer
count_filter_vec, tfidf_filter_vec = CountVectorizer(analyzer = 'word', stop_words = 'english'), TfidfVectorizer(analyzer = 'word', stop_words = 'english')

# 使用带有停用词过滤的CountVectorizer对训练和测试文本分别进行量化处理
X_count_filter_train = count_filter_vec.fit_transform(X_train)
X_count_filter_test = count_filter_vec.transform(X_test)

# 使用带有停用词过滤的TfidfVectorizer对训练和测试文本分别进行量化处理
X_tfidf_filter_train = tfidf_filter_vec.fit_transform(X_train)
X_tfidf_filter_test = tfidf_filter_vec.transform(X_test)

# 初始化默认配置的朴素贝叶斯分类器，并对CountVectorizer后的数据进行预测与准确性评估
mnb_count_filter = MultinomialNB()
mnb_count_filter.fit(X_count_filter_train, y_train)
y_count_filter_predict = mnb_count_filter.predict(X_count_filter_test)
print('使用停用词过滤的CountVectorizer准确率：', mnb_count_filter.score(X_count_filter_test, y_test))

# 初始化另一个默认配置的朴素贝叶斯分类器，并对TfidfVectorizer后的数据进行预测与准确性评估
mnb_tfidf_filter = MultinomialNB()
mnb_tfidf_filter.fit(X_tfidf_filter_train, y_train)
y_tfidf_filter_predict = mnb_tfidf_filter.predict(X_tfidf_filter_test)
print('使用停用词过滤的TfidfVectorizer准确率：', mnb_tfidf_filter.score(X_tfidf_filter_test, y_test))


# 对上述两个模型进行更加详细的性能评估
from sklearn.metrics import classification_report
print(classification_report(y_test, y_count_filter_predict, target_names = news.target_names))
print(classification_report(y_test, y_tfidf_filter_predict, target_names = news.target_names))


```

    使用停用词过滤的CountVectorizer准确率： 0.8637521222410866
    使用停用词过滤的TfidfVectorizer准确率： 0.8826400679117148
                              precision    recall  f1-score   support
    
                 alt.atheism       0.85      0.89      0.87       201
               comp.graphics       0.62      0.88      0.73       250
     comp.os.ms-windows.misc       0.93      0.22      0.36       248
    comp.sys.ibm.pc.hardware       0.62      0.88      0.73       240
       comp.sys.mac.hardware       0.93      0.85      0.89       242
              comp.windows.x       0.82      0.85      0.84       263
                misc.forsale       0.90      0.79      0.84       257
                   rec.autos       0.91      0.91      0.91       238
             rec.motorcycles       0.98      0.94      0.96       276
          rec.sport.baseball       0.98      0.92      0.95       251
            rec.sport.hockey       0.92      0.99      0.95       233
                   sci.crypt       0.91      0.97      0.93       238
             sci.electronics       0.87      0.89      0.88       249
                     sci.med       0.94      0.95      0.95       245
                   sci.space       0.91      0.96      0.93       221
      soc.religion.christian       0.87      0.94      0.90       232
          talk.politics.guns       0.89      0.96      0.93       251
       talk.politics.mideast       0.95      0.98      0.97       231
          talk.politics.misc       0.84      0.90      0.87       188
          talk.religion.misc       0.91      0.53      0.67       158
    
                 avg / total       0.88      0.86      0.85      4712
    
                              precision    recall  f1-score   support
    
                 alt.atheism       0.86      0.81      0.83       201
               comp.graphics       0.85      0.81      0.83       250
     comp.os.ms-windows.misc       0.84      0.87      0.86       248
    comp.sys.ibm.pc.hardware       0.78      0.88      0.83       240
       comp.sys.mac.hardware       0.92      0.90      0.91       242
              comp.windows.x       0.95      0.88      0.91       263
                misc.forsale       0.90      0.80      0.85       257
                   rec.autos       0.89      0.92      0.90       238
             rec.motorcycles       0.98      0.94      0.96       276
          rec.sport.baseball       0.97      0.93      0.95       251
            rec.sport.hockey       0.88      0.99      0.93       233
                   sci.crypt       0.85      0.98      0.91       238
             sci.electronics       0.93      0.86      0.89       249
                     sci.med       0.96      0.93      0.95       245
                   sci.space       0.90      0.97      0.93       221
      soc.religion.christian       0.70      0.96      0.81       232
          talk.politics.guns       0.84      0.98      0.90       251
       talk.politics.mideast       0.92      0.99      0.95       231
          talk.politics.misc       0.97      0.74      0.84       188
          talk.religion.misc       0.96      0.29      0.45       158
    
                 avg / total       0.89      0.88      0.88      4712
    
    

***对停用词进行过滤的文本特征抽取方法，平均比不使用过滤停用词的模型综合性能高出3%~4%***

## 特征筛选

***特征筛选与PCA这类通过选择主成分对特征进行重建的方法略有区别：对于PCA而言，我们经常无法解释重建之后的特征，但是特征筛选不存在对特征值的修改，而更加侧重寻找那些对模型的性能提升较大的少量特征。***

### 代码59：使用Titanic数据集，通过特征筛选的方法一步步提升决策树的预测性能


```python
import pandas as pd
# 从互联网读取titanic数据集
titanic = pd.read_csv('http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt')

# 分离数据特征与预测目标
y = titanic['survived']
X = titanic.drop(['row.names', 'name', 'survived'], axis = 1)

# 对缺失数据进行填充
X['age'].fillna(X['age'].mean(), inplace = True)
X.fillna('UNKNOWN', inplace = True)

# 分割数据，依然采用25%用于测试
from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, random_state = 33)

# 类别型特征向量化
from sklearn.feature_extraction import DictVectorizer
vec = DictVectorizer()
X_train = vec.fit_transform(X_train.to_dict(orient = 'record'))
X_test = vec.transform(X_test.to_dict(orient = 'record'))

#输出处理后特征向量的维度
print(len(vec.feature_names_))
```

    474
    


```python
# 使用决策树模型依靠所有特征进行预测，并作性能评估
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(criterion = 'entropy')
dt.fit(X_train, y_train)
dt.score(X_test, y_test)
```




    0.8206686930091185




```python
# 导入特征筛选器
from sklearn import feature_selection
# 筛选前20%的特征，使用相同配置的决策树模型进行预测，并且评估性能
fs = feature_selection.SelectPercentile(feature_selection.chi2, percentile = 20)
X_train_fs = fs.fit_transform(X_train, y_train)
dt.fit(X_train_fs, y_train)
X_test_fs = fs.transform(X_test)
dt.score(X_test_fs, y_test)
```




    0.8267477203647416




```python
# 通过交叉验证的方法，按照固定间隔的百分比筛选特征， 并做图展示性能随特征筛选比例的变化
from sklearn.cross_validation import cross_val_score
import numpy as np
percentiles = range(1, 100, 2)
results = []
for i in percentiles:
    fs = feature_selection.SelectPercentile(feature_selection.chi2, percentile = i)
    X_train_fs = fs.fit_transform(X_train, y_train)
    scores = cross_val_score(dt, X_train_fs, y_train, cv = 5)
    results = np.append(results, scores.mean())
print(results)
# 找到体现最佳性能的特征筛选百分比
opt = np.where(results == results.max())[0][0]
#print(type(opt))
print('Optimal number of features %d'%percentiles[opt])

```

    [0.85063904 0.85673057 0.87501546 0.88622964 0.86894455 0.86795506
     0.86898578 0.87201608 0.86996496 0.87504638 0.86794475 0.86893424
     0.86693465 0.86691404 0.86691404 0.86284271 0.861812   0.8598021
     0.86284271 0.8608122  0.86488353 0.86286333 0.86691404 0.86691404
     0.86589363 0.86486291 0.86895485 0.86589363 0.86792414 0.86794475
     0.86999588 0.86793445 0.87198516 0.86588332 0.86487322 0.86996496
     0.87198516 0.86792414 0.87098536 0.86895485 0.87201608 0.86689342
     0.86384251 0.86385281 0.86589363 0.86183261 0.85776129 0.86285302
     0.86284271 0.86183261]
    Optimal number of features 7
    


```python
import pylab as pl
pl.plot(percentiles, results)
pl.xlabel('percentiles of features')
pl.ylabel('accuracy')

#使用最佳筛选后的特征，利用相同配置的模型在测试集上进行性能评估
from sklearn import feature_selection
fs = feature_selection.SelectPercentile(feature_selection.chi2, percentile = 7)
X_train_fs = fs.fit_transform(X_train, y_train)
dt.fit(X_train_fs, y_train)
X_test_fs = fs.transform(X_test)
dt.score(X_test_fs, y_test)
```




    0.8571428571428571




![png](output_19_1.png)


# 3.1.2 模型正则化

***任何机器学习模型在训练集上的性能表现，都不能作为其对未知测试数据预测能力的评估。泛化力（Generalization），欠拟合和过拟合，L1范数正则化和L2范数正则化来加强模型的泛化力，避免模型参数过拟合。***

### 代码60：使用线性回归模型在比萨训练样本上进行拟合


```python
# 输入训练样本的特征以及目标值，分别存储在变量X_train和y_train中
X_train = [[6], [8], [10], [14], [18]]
y_train =[[7], [9], [13], [17.5], [18]]

from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train, y_train)

import numpy as np
#在x轴上从0至25均匀采样100个数据点
xx = np.linspace(0, 26, 100)
xx = xx.reshape(xx.shape[0], 1)
# 以上述100个数据点作为基准，预测回归直线
yy = regressor.predict(xx)

# 对回归预测到的直线进行做图
import matplotlib.pyplot as plt
plt.scatter(X_train, y_train)

plt1, = plt.plot(xx, yy, label ='Degree = 1')
plt.axis([0, 25, 0, 25])
plt.xlabel('Diameter of Pizza')
plt.ylabel('Price of Pizza')
plt.legend(handles = [plt1])
plt.show()

#输出线性回归模型在训练样本上的R-squared值
print('线性回归模型在训练样本上的R2值：', regressor.score(X_train, y_train))

```


![png](output_23_0.png)


    线性回归模型在训练样本上的R2值： 0.9100015964240102
    

### 代码61：使用2次多项式回归模型在比萨训练样本上进行拟合


```python
# 导入多项式特征产生器
from sklearn.preprocessing import PolynomialFeatures
# 映射二次多项式特征
poly2 = PolynomialFeatures(degree = 2)
X_train_poly2 = poly2.fit_transform(X_train)

#以线性回归器为基础，初始化回归模型。尽管特征的维度有提升，但是模型基础仍然是线性模型
regressor_poly2 = LinearRegression()
regressor_poly2.fit(X_train_poly2, y_train)

#从新映射绘图用x轴采样数据
xx_poly2 = poly2.transform(xx)

#使用2次多项式回归模型对应X轴采样数据进行回归预测
yy_poly2 = regressor_poly2.predict(xx_poly2)

#分别对训练数据点、线性回归直线、2次多项式回归曲线进行做图
plt.scatter(X_train, y_train)

plt1, = plt.plot(xx, yy, label = 'Degree = 1')
plt2, = plt.plot(xx, yy_poly2, label = 'Degree = 2')

plt.axis([0, 25, 0, 25])
plt.xlabel('Diameter of Pizza')
plt.ylabel('Price of Pizza')
plt.legend(handles = [plt1, plt2])
plt.show()

#输出二次多项式回归模型在训练样本上的R2值
print('二次多项式回归模型在训练样本上的R2值：',regressor_poly2.score(X_train_poly2, y_train))

```


![png](output_25_0.png)


    二次多项式回归模型在训练样本上的R2值： 0.9816421639597427
    

### 代码62：使用4次多项式回归模型在比萨训练样本上进行拟合


```python
# 导入多项式特征产生器
from sklearn.preprocessing import PolynomialFeatures
# 映射4次多项式特征
poly4 = PolynomialFeatures(degree = 4)
X_train_poly4 = poly4.fit_transform(X_train)

#以线性回归器为基础，初始化回归模型。尽管特征的维度有提升，但是模型基础仍然是线性模型
regressor_poly4 = LinearRegression()
regressor_poly4.fit(X_train_poly4, y_train)

#从新映射绘图用x轴采样数据
xx_poly4 = poly4.transform(xx)

#使用4次多项式回归模型对应X轴采样数据进行回归预测
yy_poly4 = regressor_poly4.predict(xx_poly4)

#分别对训练数据点、线性回归直线、4次多项式回归曲线进行做图
plt.scatter(X_train, y_train)

plt1, = plt.plot(xx, yy, label = 'Degree = 1')
plt2, = plt.plot(xx, yy_poly2, label = 'Degree = 2')
plt4, = plt.plot(xx, yy_poly4, label = 'Degree =4')
plt.axis([0, 25, 0, 25])
plt.xlabel('Diameter of Pizza')
plt.ylabel('Price of Pizza')
plt.legend(handles = [plt1, plt2, plt4])
plt.show()

#输出4次多项式回归模型在训练样本上的R2值
print('4次多项式回归模型在训练样本上的R2值：',regressor_poly4.score(X_train_poly4, y_train))

```


![png](output_27_0.png)


    4次多项式回归模型在训练样本上的R2值： 1.0
    

### 代码63：评估3种回归模型在测试数据集上的性能表现


```python
# 准备测试数据
X_test = [[6], [8], [11], [16]]
y_test = [[8], [12], [15], [18]]

# 验证测试数据
regressor.score(X_test, y_test)
```




    0.809726797707665




```python
# 2次多项式性能
X_test_poly2 = poly2.transform(X_test)
regressor_poly2.score(X_test_poly2, y_test)
```




    0.8675443656345054




```python
# 4次多项式性能
X_test_poly4 = poly4.transform(X_test)
regressor_poly4.score(X_test_poly4, y_test)
```




    0.8095880795781909



### 3.1.2.2 L1范数正则化


```python
# 代码64：Lasso模型在4次多项式特征上的拟合表现
from sklearn.linear_model import Lasso
#使用默认配置初始化Lasso
lasso_poly4 = Lasso()
# 拟合
lasso_poly4.fit(X_train_poly4, y_train)
# Lasso模型性能评估
print(lasso_poly4.score(X_test_poly4, y_test))
```

    0.8388926873604381
    

    C:\ProgramData\Anaconda3\lib\site-packages\sklearn\linear_model\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.
      ConvergenceWarning)
    


```python
#输出Lasso模型参数列表
print(lasso_poly4.coef_)
```

    [ 0.00000000e+00  0.00000000e+00  1.17900534e-01  5.42646770e-05
     -2.23027128e-04]
    


```python
# 回顾普通4次多项式回归模型过拟合之后的性能
print(regressor_poly4.score(X_test_poly4, y_test))
```

    0.8095880795781909
    


```python
# 回顾普通4次多项式回归模型的参数列表
print(regressor_poly4.coef_)
```

    [[ 0.00000000e+00 -2.51739583e+01  3.68906250e+00 -2.12760417e-01
       4.29687500e-03]]
    

***Lasso的特点：  ***
1. 相比于普通4次多项式回归模型在测试集上的表现，默认配置的Lasso模型性能提高了大约3%；  
2. 相较之下，Lasso模型拟合后的参数列表中，4次与3次特征的参数均为0.0， 使得特征更为稀疏。

### 3.1.2.3 L2范数正则化


```python
# 代码65：Ridge模型在4次多项式特征上的拟合表现
#输出普通4次多项式回归模型的参数列表
print(regressor_poly4.coef_)
```

    [[ 0.00000000e+00 -2.51739583e+01  3.68906250e+00 -2.12760417e-01
       4.29687500e-03]]
    


```python
# 输出上述这些参数的平方和，验证参数之间的巨大差异
print(np.sum(regressor_poly4.coef_**2))
```

    647.3826456921447
    


```python
# 导入Ridge模型
from sklearn.linear_model import Ridge
# 使用默认配置初始化Ridge
ridge_poly4 = Ridge()
#拟合
ridge_poly4.fit(X_train_poly4, y_train)
# 输出Ridge模型在测试样本上的回归性能
print(ridge_poly4.score(X_test_poly4, y_test))
```

    0.8374201759366504
    


```python
# 输出Ridge模型参数列表，观察参数差异
print(ridge_poly4.coef_)
```

    [[ 0.         -0.00492536  0.12439632 -0.00046471 -0.00021205]]
    


```python
# 计算Ridge模型拟合后参数的平方和
print(np.sum(ridge_poly4.coef_**2))
```

    0.015498965203562037
    

***Ridge模型特点：***
1. 相比于普通4次多项式回归模型在测试集上的表现，Ridge模型性能提高了近3%；
2. 与普通4次多项式回归模型不同的是，Ridge模型拟合后的参数之间差异非常小。

# 3.1.3 模型检验


```python
# 留一验证和K折交叉验证
```

# 3.1.4 超参数搜索

### 3.1.4.1 网格搜索


```python
# 代码66：使用单线程对文本分类的朴素贝叶斯模型的超参数组合执行网格搜索
# 导入20类新闻文本抓取器
from sklearn.datasets import fetch_20newsgroups
import numpy as np

news = fetch_20newsgroups(subset = 'all')

#分割数据
from sklearn.cross_validation import train_test_split

# 对前3000条新闻文本进行数据分割，25%文本用于未来测试
X_train, X_test, y_train, y_test = train_test_split(news.data[:3000], news.target[:3000], test_size = 0.25, random_state = 33)

# 导入支持向量机（分类）模型
from sklearn.svm import SVC

# 导入TfidfVectorizer文本抽取器
from sklearn.feature_extraction.text import TfidfVectorizer

#导入Pipeline
from sklearn.pipeline import Pipeline

# 使用Pipeline简化系统搭建流程，将文本抽取和分类器模型串联起来
clf = Pipeline([('vect', TfidfVectorizer(stop_words = 'english', analyzer = 'word')), ('svc', SVC())])

#这里需要试验的2个超参数的个数分别是4、3，svc_gamma的参数共有10^-2,10^-1...这样我们一共有12种超参数组合，12个不同参数下的模型
parameters = {'svc__gamma': np.logspace(-2, 1, 4), 'svc__C': np.logspace(-1, 1, 3)}

# 导入网格搜索模块
from sklearn.model_selection import GridSearchCV

# 将12组参数组合以及初始化的Pipeline包括3折交叉验证的要求全部告知GridSearchCV， 请注意refit = True这一设定。
gs = GridSearchCV(clf, parameters, verbose = 2, refit = True, cv = 3)

# 执行单线程网格搜索
gs.fit(X_train, y_train)
%time gs.best_params_, gs.best_score_

# 输出最佳模型在测试集上的准确性
print(gs.score(X_test, y_test))
```

    Fitting 3 folds for each of 12 candidates, totalling 36 fits
    [CV] svc__C=0.1, svc__gamma=0.01 .....................................
    [CV] ...................... svc__C=0.1, svc__gamma=0.01, total=   5.6s
    [CV] svc__C=0.1, svc__gamma=0.01 .....................................
    

    [Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    8.4s remaining:    0.0s
    

    [CV] ...................... svc__C=0.1, svc__gamma=0.01, total=   5.7s
    [CV] svc__C=0.1, svc__gamma=0.01 .....................................
    [CV] ...................... svc__C=0.1, svc__gamma=0.01, total=   5.9s
    [CV] svc__C=0.1, svc__gamma=0.1 ......................................
    [CV] ....................... svc__C=0.1, svc__gamma=0.1, total=   5.6s
    [CV] svc__C=0.1, svc__gamma=0.1 ......................................
    [CV] ....................... svc__C=0.1, svc__gamma=0.1, total=   5.7s
    [CV] svc__C=0.1, svc__gamma=0.1 ......................................
    [CV] ....................... svc__C=0.1, svc__gamma=0.1, total=   5.8s
    [CV] svc__C=0.1, svc__gamma=1.0 ......................................
    [CV] ....................... svc__C=0.1, svc__gamma=1.0, total=   5.7s
    [CV] svc__C=0.1, svc__gamma=1.0 ......................................
    [CV] ....................... svc__C=0.1, svc__gamma=1.0, total=   5.8s
    [CV] svc__C=0.1, svc__gamma=1.0 ......................................
    [CV] ....................... svc__C=0.1, svc__gamma=1.0, total=   5.9s
    [CV] svc__C=0.1, svc__gamma=10.0 .....................................
    [CV] ...................... svc__C=0.1, svc__gamma=10.0, total=   6.0s
    [CV] svc__C=0.1, svc__gamma=10.0 .....................................
    [CV] ...................... svc__C=0.1, svc__gamma=10.0, total=   6.2s
    [CV] svc__C=0.1, svc__gamma=10.0 .....................................
    [CV] ...................... svc__C=0.1, svc__gamma=10.0, total=   6.3s
    [CV] svc__C=1.0, svc__gamma=0.01 .....................................
    [CV] ...................... svc__C=1.0, svc__gamma=0.01, total=   5.6s
    [CV] svc__C=1.0, svc__gamma=0.01 .....................................
    [CV] ...................... svc__C=1.0, svc__gamma=0.01, total=   5.7s
    [CV] svc__C=1.0, svc__gamma=0.01 .....................................
    [CV] ...................... svc__C=1.0, svc__gamma=0.01, total=   5.9s
    [CV] svc__C=1.0, svc__gamma=0.1 ......................................
    [CV] ....................... svc__C=1.0, svc__gamma=0.1, total=   5.6s
    [CV] svc__C=1.0, svc__gamma=0.1 ......................................
    [CV] ....................... svc__C=1.0, svc__gamma=0.1, total=   5.7s
    [CV] svc__C=1.0, svc__gamma=0.1 ......................................
    [CV] ....................... svc__C=1.0, svc__gamma=0.1, total=   5.8s
    [CV] svc__C=1.0, svc__gamma=1.0 ......................................
    [CV] ....................... svc__C=1.0, svc__gamma=1.0, total=   5.7s
    [CV] svc__C=1.0, svc__gamma=1.0 ......................................
    [CV] ....................... svc__C=1.0, svc__gamma=1.0, total=   5.8s
    [CV] svc__C=1.0, svc__gamma=1.0 ......................................
    [CV] ....................... svc__C=1.0, svc__gamma=1.0, total=   5.9s
    [CV] svc__C=1.0, svc__gamma=10.0 .....................................
    [CV] ...................... svc__C=1.0, svc__gamma=10.0, total=   5.8s
    [CV] svc__C=1.0, svc__gamma=10.0 .....................................
    [CV] ...................... svc__C=1.0, svc__gamma=10.0, total=   5.8s
    [CV] svc__C=1.0, svc__gamma=10.0 .....................................
    [CV] ...................... svc__C=1.0, svc__gamma=10.0, total=   5.9s
    [CV] svc__C=10.0, svc__gamma=0.01 ....................................
    [CV] ..................... svc__C=10.0, svc__gamma=0.01, total=   5.6s
    [CV] svc__C=10.0, svc__gamma=0.01 ....................................
    [CV] ..................... svc__C=10.0, svc__gamma=0.01, total=   5.7s
    [CV] svc__C=10.0, svc__gamma=0.01 ....................................
    [CV] ..................... svc__C=10.0, svc__gamma=0.01, total=   5.8s
    [CV] svc__C=10.0, svc__gamma=0.1 .....................................
    [CV] ...................... svc__C=10.0, svc__gamma=0.1, total=   5.7s
    [CV] svc__C=10.0, svc__gamma=0.1 .....................................
    [CV] ...................... svc__C=10.0, svc__gamma=0.1, total=   5.8s
    [CV] svc__C=10.0, svc__gamma=0.1 .....................................
    [CV] ...................... svc__C=10.0, svc__gamma=0.1, total=   5.8s
    [CV] svc__C=10.0, svc__gamma=1.0 .....................................
    [CV] ...................... svc__C=10.0, svc__gamma=1.0, total=   5.8s
    [CV] svc__C=10.0, svc__gamma=1.0 .....................................
    [CV] ...................... svc__C=10.0, svc__gamma=1.0, total=   5.8s
    [CV] svc__C=10.0, svc__gamma=1.0 .....................................
    [CV] ...................... svc__C=10.0, svc__gamma=1.0, total=   5.9s
    [CV] svc__C=10.0, svc__gamma=10.0 ....................................
    [CV] ..................... svc__C=10.0, svc__gamma=10.0, total=   5.8s
    [CV] svc__C=10.0, svc__gamma=10.0 ....................................
    [CV] ..................... svc__C=10.0, svc__gamma=10.0, total=   5.9s
    [CV] svc__C=10.0, svc__gamma=10.0 ....................................
    [CV] ..................... svc__C=10.0, svc__gamma=10.0, total=   6.0s
    

    [Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed:  5.3min finished
    

    Wall time: 0 ns
    0.8226666666666667
    

### 3.1.4.2 并行搜索


```python
# 代码67：使用多个线程对文本分类的朴素贝叶斯模型的超参数组合执行并行化的网格搜索
# 代码66：使用单线程对文本分类的朴素贝叶斯模型的超参数组合执行网格搜索
# 导入20类新闻文本抓取器
from sklearn.datasets import fetch_20newsgroups
import numpy as np

news = fetch_20newsgroups(subset = 'all')

#分割数据
from sklearn.cross_validation import train_test_split

# 对前3000条新闻文本进行数据分割，25%文本用于未来测试
X_train, X_test, y_train, y_test = train_test_split(news.data[:3000], news.target[:3000], test_size = 0.25, random_state = 33)

# 导入支持向量机（分类）模型
from sklearn.svm import SVC

# 导入TfidfVectorizer文本抽取器
from sklearn.feature_extraction.text import TfidfVectorizer

#导入Pipeline
from sklearn.pipeline import Pipeline

# 使用Pipeline简化系统搭建流程，将文本抽取和分类器模型串联起来
clf = Pipeline([('vect', TfidfVectorizer(stop_words = 'english', analyzer = 'word')), ('svc', SVC())])

#这里需要试验的2个超参数的个数分别是4、3，svc_gamma的参数共有10^-2,10^-1...这样我们一共有12种超参数组合，12个不同参数下的模型
parameters = {'svc__gamma': np.logspace(-2, 1, 4), 'svc__C': np.logspace(-1, 1, 3)}

# 导入网格搜索模块
from sklearn.model_selection import GridSearchCV

# 初始化配置并行网格搜索，n_jobs = -1代表使用该计算机的全部CPU。
gs_bx = GridSearchCV(clf, parameters, verbose = 2, refit = True, cv = 3, n_jobs = -1)

# 执行多线程并行网格搜索
gs_bx.fit(X_train, y_train)
%time gs_bx.best_params_, gs_bx.best_score_

# 输出最佳模型在测试集上的准确性
print(gs_bx.score(X_test, y_test))

```

    Fitting 3 folds for each of 12 candidates, totalling 36 fits
    

    [Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:  2.1min finished
    

    Wall time: 0 ns
    0.8226666666666667
    
